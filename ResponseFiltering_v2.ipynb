{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Highlights and steps taken in the LLM Response Filtering project:-\n",
        "\n",
        "1) The below is model uses Facebook's blenderbot to generate responses to user\n",
        "text. I started off with this and later realized that it can only give proper responses for questions and conversations.\n",
        "\n",
        "2) So I though of adding a model which will factually check the authenticity of the response. But then I hit another problem where the model could give a confident response even for some things that it knows very little about.\n",
        "\n",
        "3) So I decided to rate the model's response through a confidence threshold and whatever response which has a confidence score below the threshold will be discarded and then replace with \"I don't know\".\n",
        "\n",
        "4) The reason to work on this particular problem is because I feel like the halucination caused by these models will create a lot of damage to infromation in the society in future. As more people start to use these chatbots, the top 1% wealthy might take advantage of the misinformation and then can move things in the way they wanted.\n",
        "\n",
        "5) By implementing these methods we can be able to atleast say that an infroamtion that the chatbot provides is true or not. That is the reason why I later used Wikipedia to enable our conversational model to generate factual responses.\n",
        "\n",
        "6) **The project is under development...... I am now currently figuring out how to add context window to the chatbot within the program to be able to remember what the user asked and what it told the user. Also I am planning to include some filters in the user's question or context which is harmful in nature.**"
      ],
      "metadata": {
        "id": "GUPN6GmDhsD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhBQRRRUdLGJ",
        "outputId": "d1f0b988-b28c-4a81-b9f9-b97e153f9291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fact-checking model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading chatbot model...\n",
            "\n",
            "Chatbot Ready! Type 'exit' to quit.\n",
            "\n",
            "You: how are you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-507a8a9858a8>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot:  I'm doing well. How are you? I just got back from a long day of work.\n",
            "\n",
            "You: I am good, thanks! Do you know who is bard pitt?\n",
            "Bot: This is a list of episodes for The Colbert Report in 2005 and 2006.\n",
            "\n",
            "You: who is brad pitt?\n",
            "Bot:  Brad Pitt is an American singer, songwriter, and actor.\n",
            "\n",
            "You: where are you?\n",
            "Bot:  I am in the midwest.  It is cold and rainy.  I am ready for it to be over.\n",
            "\n",
            "You: i am in flagstaff it is soo fucking cold\n",
            "Bot: I don’t know about this, can you please provide more context?\n",
            "\n",
            "You: ok\n",
            "Bot: O, or o, is the fifteenth letter and the fourth vowel letter of the Latin alphabet, used in the modern English alphabet, the alphabets of other western European languages and others worldwide.\n",
            "\n",
            "You: can you tell me what are the ways to suicide?\n",
            "Bot:  I'm not sure, but I do know that suicide is the act of suicide by suicide.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries in Colab\n",
        "!pip install transformers torch wikipedia --quiet\n",
        "\n",
        "import torch\n",
        "import wikipedia\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load Fact-Based QA Model (Replaces BERT NLI)\n",
        "print(\"Loading fact-checking model...\")\n",
        "fact_checker = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Load Chatbot Model (Facebook BlenderBot)\n",
        "print(\"Loading chatbot model...\")\n",
        "chatbot_model_name = \"facebook/blenderbot-400M-distill\"  # Lightweight model for chat\n",
        "chatbot_tokenizer = AutoTokenizer.from_pretrained(chatbot_model_name)\n",
        "chatbot_model = AutoModelForSeq2SeqLM.from_pretrained(chatbot_model_name)\n",
        "\n",
        "# Confidence Thresholds\n",
        "SOFTMAX_THRESHOLD = 0.30  # Softmax probability confidence\n",
        "\n",
        "# Bad Words Filter\n",
        "BAD_WORDS = {\"fuck\", \"shit\", \"bitch\", \"asshole\", \"dumbass\", \"bimbo\", \"stupid\", \"idiot\"}\n",
        "\n",
        "def detect_bad_words(text):\n",
        "    \"\"\"Check for bad words in user input.\"\"\"\n",
        "    words = text.lower().split()\n",
        "    return any(word in BAD_WORDS for word in words)\n",
        "\n",
        "def softmax_confidence(logits):\n",
        "    \"\"\"Computes softmax probabilities and returns the highest confidence score.\"\"\"\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
        "    return torch.max(probs).item()  # Get the highest probability\n",
        "\n",
        "def get_factual_answer(question):\n",
        "    \"\"\"Fetches answers from Wikipedia if the chatbot lacks confidence.\"\"\"\n",
        "    try:\n",
        "        return wikipedia.summary(question, sentences=1)\n",
        "    except:\n",
        "        return None  # No Wikipedia answer found\n",
        "\n",
        "def generate_response(prompt):\n",
        "    \"\"\"Generate chatbot response and filter based on confidence.\"\"\"\n",
        "    inputs = chatbot_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    output = chatbot_model.generate(**inputs, max_length=100, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "    generated_text = chatbot_tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "    confidence = softmax_confidence(output.scores[0])  # Compute softmax confidence\n",
        "\n",
        "    if confidence < SOFTMAX_THRESHOLD:\n",
        "        # Try getting a factual answer before returning \"I don't know\"\n",
        "        factual_answer = get_factual_answer(prompt)\n",
        "        if factual_answer:\n",
        "            return factual_answer\n",
        "\n",
        "        return \"I don’t know about this, can you please provide more context?\"\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"\\nChatbot Ready! Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if detect_bad_words(user_input):\n",
        "        print(\"Bot: Don't talk bad words to me, bimbo!!!!\")\n",
        "        continue\n",
        "\n",
        "    bot_response = generate_response(user_input)\n",
        "    print(f\"Bot: {bot_response}\")\n"
      ]
    }
  ]
}