{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Highlights and steps taken in the LLM Response Filtering project:-\n",
        "\n",
        "1) The below is model uses Facebook's blenderbot to generate responses to user\n",
        "text. I started off with this and later realized that it can only give proper responses for questions and conversations.\n",
        "\n",
        "2) So I though of adding a model which will factually check the authenticity of the response. But then I hit another problem where the model could give a confident response even for some things that it knows very little about.\n",
        "\n",
        "3) So I decided to rate the model's response through a confidence threshold and whatever response which has a confidence score below the threshold will be discarded and then replace with \"I don't know\".\n",
        "\n",
        "4) The reason to work on this particular problem is because I feel like the halucination caused by these models will create a lot of damage to infromation in the society in future. As more people start to use these chatbots, the top 1% wealthy might take advantage of the misinformation and then can move things in the way they wanted.\n",
        "\n",
        "5) By implementing these methods we can be able to atleast say that an infroamtion that the chatbot provides is true or not. That is the reason why I later used Wikipedia to enable our conversational model to generate factual responses.\n",
        "\n",
        "6) **The project is under development...... I am now currently figuring out how to add context window to the chatbot within the program to be able to remember what the user asked and what it told the user.**"
      ],
      "metadata": {
        "id": "GUPN6GmDhsD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "bhBQRRRUdLGJ",
        "outputId": "ac56bfc4-c971-4428-8666-be082594284a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fact-checking model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading chatbot model...\n",
            "\n",
            "Chatbot Ready! Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-507a8a9858a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Install required libraries in Colab\n",
        "!pip install transformers torch wikipedia --quiet\n",
        "\n",
        "import torch\n",
        "import wikipedia\n",
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load Fact-Based QA Model (Replaces BERT NLI)\n",
        "print(\"Loading fact-checking model...\")\n",
        "fact_checker = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Load Chatbot Model (Facebook BlenderBot)\n",
        "print(\"Loading chatbot model...\")\n",
        "chatbot_model_name = \"facebook/blenderbot-400M-distill\"  # Lightweight model for chat\n",
        "chatbot_tokenizer = AutoTokenizer.from_pretrained(chatbot_model_name)\n",
        "chatbot_model = AutoModelForSeq2SeqLM.from_pretrained(chatbot_model_name)\n",
        "\n",
        "# Confidence Thresholds\n",
        "SOFTMAX_THRESHOLD = 0.30  # Softmax probability confidence\n",
        "\n",
        "# Bad Words Filter\n",
        "BAD_WORDS = {\"fuck\", \"shit\", \"bitch\", \"asshole\", \"dumbass\", \"bimbo\", \"stupid\", \"idiot\"}\n",
        "\n",
        "def detect_bad_words(text):\n",
        "    \"\"\"Check for bad words in user input.\"\"\"\n",
        "    words = text.lower().split()\n",
        "    return any(word in BAD_WORDS for word in words)\n",
        "\n",
        "def softmax_confidence(logits):\n",
        "    \"\"\"Computes softmax probabilities and returns the highest confidence score.\"\"\"\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
        "    return torch.max(probs).item()  # Get the highest probability\n",
        "\n",
        "def get_factual_answer(question):\n",
        "    \"\"\"Fetches answers from Wikipedia if the chatbot lacks confidence.\"\"\"\n",
        "    try:\n",
        "        return wikipedia.summary(question, sentences=1)\n",
        "    except:\n",
        "        return None  # No Wikipedia answer found\n",
        "\n",
        "def generate_response(prompt):\n",
        "    \"\"\"Generate chatbot response and filter based on confidence.\"\"\"\n",
        "    inputs = chatbot_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    output = chatbot_model.generate(**inputs, max_length=100, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "    generated_text = chatbot_tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
        "    confidence = softmax_confidence(output.scores[0])  # Compute softmax confidence\n",
        "\n",
        "    if confidence < SOFTMAX_THRESHOLD:\n",
        "        # Try getting a factual answer before returning \"I don't know\"\n",
        "        factual_answer = get_factual_answer(prompt)\n",
        "        if factual_answer:\n",
        "            return factual_answer\n",
        "\n",
        "        return \"I donâ€™t know about this, can you please provide more context?\"\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "print(\"\\nChatbot Ready! Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if detect_bad_words(user_input):\n",
        "        print(\"Bot: Don't talk bad words to me, bimbo!!!!\")\n",
        "        continue\n",
        "\n",
        "    bot_response = generate_response(user_input)\n",
        "    print(f\"Bot: {bot_response}\")\n"
      ]
    }
  ]
}